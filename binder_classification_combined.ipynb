{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e2d1f47",
   "metadata": {},
   "source": [
    "# Binder classification with combined SMILES fingerprints and RDKit descriptors\n",
    "\n",
    "This notebook merges the previously separate SMILES-fingerprint and RDKit-descriptor workflows. It loads the curated IC50 dataset, cleans it, generates Morgan fingerprints plus RDKit descriptors for each molecule, aligns compounds that have both representations, and evaluates classification models on the concatenated feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5554e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ.setdefault('OMP_NUM_THREADS', '1')\n",
    "os.environ.setdefault('RDKIT_MAX_THREADS', '1')\n",
    "os.environ.setdefault('RDKIT_DISABLE_THREADS', '1')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    auc,\n",
    "    matthews_corrcoef,\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import clone\n",
    "from IPython.display import display\n",
    "\n",
    "plt.rcParams.update({\"figure.figsize\": (8, 5), \"axes.grid\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e4c08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean the IC50 dataset\n",
    "\n",
    "df = pd.read_csv(\"ic50.tsv\", sep=\"\t\", low_memory=False)\n",
    "df[\"Standard Value\"] = pd.to_numeric(df[\"Standard Value\"], errors=\"coerce\")\n",
    "\n",
    "df = df.dropna(subset=[\"Standard Value\", \"Smiles\"]).copy()\n",
    "df[\"Binder\"] = (df[\"Standard Value\"] <= 2000).astype(int)\n",
    "\n",
    "unwanted_patterns = [\n",
    "    \".C\", \".Cl\", \".NA+\", \".Na+\", \".na+\", \"[Na+]\", \"Cl.\", \".O=C(O)C(F)(F)F\"\n",
    "]\n",
    "mask = pd.Series(False, index=df.index)\n",
    "for pattern in unwanted_patterns:\n",
    "    mask |= df[\"Smiles\"].str.contains(pattern, regex=False, na=False)\n",
    "removed = int(mask.sum())\n",
    "if removed:\n",
    "    df = df[~mask].copy()\n",
    "    print(f\"Removed {removed} compounds due to unwanted ion patterns\")\n",
    "\n",
    "dupe_mask = df.duplicated(subset=\"Smiles\", keep=\"first\")\n",
    "removed_dupes = int(dupe_mask.sum())\n",
    "if removed_dupes:\n",
    "    df = df[~dupe_mask].copy()\n",
    "    print(f\"Removed {removed_dupes} duplicate SMILES entries\")\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "print(f\"Dataset size after cleaning: {len(df)} compounds\")\n",
    "print(f\"Binder ratio (<=2000 nM): {df['Binder'].mean()*100:.2f}%\")\n",
    "print(\"Sample molecules:\")\n",
    "for i, (_, row) in enumerate(df.head(5).iterrows(), 1):\n",
    "    smiles_str = row['Smiles'][:70] + \"...\" if len(row['Smiles']) > 70 else row['Smiles']\n",
    "    print(f\"  {i}. {smiles_str} | Binder: {row['Binder']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e1c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Morgan fingerprint generation\n",
    "\n",
    "morgan_generator = GetMorganGenerator(radius=2, fpSize=4096)\n",
    "\n",
    "def smiles_to_morgan_bits(smiles: str):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    try:\n",
    "        fp = morgan_generator.GetFingerprint(mol)\n",
    "        return np.array(fp, dtype=np.int8)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "fingerprint_rows = []\n",
    "fingerprint_indices = []\n",
    "print(\"Converting SMILES to 4096-bit Morgan fingerprints...\")\n",
    "for idx, smiles in df['Smiles'].items():\n",
    "    fp = smiles_to_morgan_bits(smiles)\n",
    "    if fp is not None:\n",
    "        fingerprint_rows.append(fp)\n",
    "        fingerprint_indices.append(idx)\n",
    "\n",
    "X_fp = np.asarray(fingerprint_rows)\n",
    "y_fp = df.loc[fingerprint_indices, 'Binder'].to_numpy()\n",
    "print(f\"Valid fingerprint count: {X_fp.shape[0]} (out of {len(df)})\")\n",
    "print(f\"Fingerprint matrix shape: {X_fp.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ab52fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDKit descriptor calculation\n",
    "\n",
    "descriptor_names = [name for name, _ in Descriptors._descList]\n",
    "descriptor_calculator = MoleculeDescriptors.MolecularDescriptorCalculator(descriptor_names)\n",
    "\n",
    "def smiles_to_descriptors(smiles: str):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    try:\n",
    "        return descriptor_calculator.CalcDescriptors(mol)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "print(\"Calculating RDKit descriptors...\")\n",
    "descriptor_rows = []\n",
    "descriptor_indices = []\n",
    "for idx, smiles in df['Smiles'].items():\n",
    "    values = smiles_to_descriptors(smiles)\n",
    "    if values is not None:\n",
    "        descriptor_rows.append(values)\n",
    "        descriptor_indices.append(idx)\n",
    "\n",
    "descriptor_df = (pd.DataFrame(descriptor_rows, columns=descriptor_names, index=descriptor_indices)\n",
    "                 .replace([np.inf, -np.inf], np.nan))\n",
    "descriptor_df = descriptor_df.dropna(axis=1, how='all')\n",
    "descriptor_df = descriptor_df.fillna(descriptor_df.median())\n",
    "float32_limit = np.finfo(np.float32).max\n",
    "large_mask = descriptor_df.abs() >= float32_limit\n",
    "if large_mask.any().any():\n",
    "    bad_cols = descriptor_df.columns[large_mask.any(axis=0)].tolist()\n",
    "    descriptor_df = descriptor_df.drop(columns=bad_cols)\n",
    "    print(f\"Dropped {len(bad_cols)} descriptor(s) exceeding float32 limits: {bad_cols}\")\n",
    "\n",
    "descriptor_df = descriptor_df.astype(np.float64)\n",
    "print(f\"Descriptor matrix shape: {descriptor_df.shape}\")\n",
    "print(f\"Valid descriptor count: {descriptor_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea17485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align molecules with both fingerprints and descriptors, then combine features\n",
    "\n",
    "fp_idx_map = {idx: pos for pos, idx in enumerate(fingerprint_indices)}\n",
    "desc_idx_map = {idx: pos for pos, idx in enumerate(descriptor_df.index.tolist())}\n",
    "shared_indices = sorted(set(fp_idx_map) & set(desc_idx_map))\n",
    "print(f\"Molecules with both feature sets: {len(shared_indices)}\")\n",
    "\n",
    "X_fp_shared = X_fp[[fp_idx_map[idx] for idx in shared_indices]]\n",
    "X_desc_shared = descriptor_df.loc[shared_indices].to_numpy()\n",
    "y_shared = df.loc[shared_indices, 'Binder'].to_numpy()\n",
    "\n",
    "X_combined = np.hstack([X_fp_shared, X_desc_shared])\n",
    "desc_start = X_fp_shared.shape[1]\n",
    "print(f\"Combined feature matrix shape: {X_combined.shape}\")\n",
    "print(f\"Fingerprint block: {X_fp_shared.shape[1]} columns, descriptor block: {X_desc_shared.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b1db3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split and baseline Random Forest evaluation\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_combined,\n",
    "    y_shared,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=y_shared,\n",
    ")\n",
    "\n",
    "rf_combined = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    max_depth=24,\n",
    "    min_samples_leaf=2,\n",
    ")\n",
    "rf_combined.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_combined.predict(X_test)\n",
    "y_prob = rf_combined.predict_proba(X_test)[:, 1]\n",
    "\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_prob)\n",
    "pr_auc = auc(recall_curve, precision_curve)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Random Forest performance on holdout split (combined features)\")\n",
    "print(f\"PR-AUC: {pr_auc:.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"MCC: {mcc:.4f}\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Balanced Accuracy: {bal_acc:.4f}\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Non-binder\", \"Binder\"]))\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(recall_curve, precision_curve, label=f\"PR curve (AUC={pr_auc:.3f})\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall curve — Random Forest (combined features)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (AUC={roc_auc:.3f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.4)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC curve — Random Forest (combined features)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c72dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated comparison across multiple classifier families\n",
    "\n",
    "def _positive_class_probs(model, X):\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return model.predict_proba(X)[:, 1]\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        raw = model.decision_function(X)\n",
    "        return 1.0 / (1.0 + np.exp(-raw))\n",
    "    return model.predict(X).astype(float)\n",
    "\n",
    "def _metric_summary(y_true, y_pred, y_prob):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "    return {\n",
    "        \"PR_AUC\": auc(recall, precision),\n",
    "        \"ROC_AUC\": roc_auc_score(y_true, y_prob),\n",
    "        \"MCC\": matthews_corrcoef(y_true, y_pred),\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "def run_stratified_kfold(model, X, y, n_splits=5, random_state=42):\n",
    "    splitter = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    fold_metrics = []\n",
    "    for train_idx, test_idx in splitter.split(X, y):\n",
    "        estimator = clone(model)\n",
    "        estimator.fit(X[train_idx], y[train_idx])\n",
    "        y_pred = estimator.predict(X[test_idx])\n",
    "        y_prob = _positive_class_probs(estimator, X[test_idx])\n",
    "        fold_metrics.append(_metric_summary(y[test_idx], y_pred, y_prob))\n",
    "    return {metric: np.mean([fold[metric] for fold in fold_metrics]) for metric in fold_metrics[0]}\n",
    "\n",
    "model_registry = {\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        max_depth=24,\n",
    "        min_samples_leaf=2,\n",
    "    ),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=600,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=1,\n",
    "    ),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(\n",
    "        random_state=42,\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=400,\n",
    "        max_depth=3,\n",
    "    ),\n",
    "    \"LogisticRegression\": make_pipeline(\n",
    "        StandardScaler(with_mean=False),\n",
    "        LogisticRegression(\n",
    "            penalty='elasticnet',\n",
    "            l1_ratio=0.3,\n",
    "            solver='saga',\n",
    "            max_iter=4000,\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "    ),\n",
    "    \"GaussianNB\": make_pipeline(\n",
    "        StandardScaler(with_mean=False),\n",
    "        GaussianNB(var_smoothing=1e-9)\n",
    "    )\n",
    "}\n",
    "\n",
    "fold_options = (5, 10)\n",
    "cv_rows = []\n",
    "for model_name, estimator in model_registry.items():\n",
    "    for folds in fold_options:\n",
    "        metrics = run_stratified_kfold(estimator, X_combined, y_shared, n_splits=folds)\n",
    "        metrics.update({\"Model\": model_name, \"Folds\": folds})\n",
    "        cv_rows.append(metrics)\n",
    "\n",
    "cv_df = (pd.DataFrame(cv_rows)\n",
    "         .sort_values(by=[\"Model\", \"Folds\"])         .reset_index(drop=True))\n",
    "print(\"Cross-validated combined-feature model comparison:\")\n",
    "try:\n",
    "    display(cv_df)\n",
    "except Exception as exc:\n",
    "    print(\"Display unavailable:\", exc)\n",
    "    print(cv_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
