{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c072b2f",
   "metadata": {},
   "source": [
    "# Binder classification with best practices\n",
    "\n",
    "This notebook revisits the combined SMILES fingerprint + RDKit descriptor workflow and incorporates best practices discussed during the review: data leakage is prevented via pipelines, feature failures are logged for inspection, models account for class imbalance, and nested cross-validation with hyperparameter search is used for unbiased comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "424510ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ.setdefault('OMP_NUM_THREADS', '1')\n",
    "os.environ.setdefault('RDKIT_MAX_THREADS', '1')\n",
    "os.environ.setdefault('RDKIT_DISABLE_THREADS', '1')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    auc,\n",
    "    matthews_corrcoef,\n",
    "    balanced_accuracy_score,\n",
    "    accuracy_score,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "plt.rcParams.update({\"figure.figsize\": (8, 5), \"axes.grid\": True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "625a4a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 25 compounds due to unwanted ion patterns\n",
      "Removed 787 duplicate SMILES entries\n",
      "Dataset size after cleaning: 2039 compounds\n",
      "Binder prevalence (<=2000 nM): 64.64%\n",
      "Sample molecules:\n",
      "  1. CN(C)OC(=O)CCC(=O)O | Binder: 0\n",
      "  2. CC(=O)CC(=O)CCC(=O)O | Binder: 0\n",
      "  3. Cc1cccc(C)c1Oc1cc2c(N3CCCC3)nc(-n3cc(C(=O)O)cn3)nc2cc1F | Binder: 1\n",
      "  4. N#Cc1cccc(NC(=O)c2ccc3cccnc3c2O)c1 | Binder: 0\n",
      "  5. COc1ccc(CNC(=O)c2ccc3cccnc3c2O)cc1 | Binder: 0\n"
     ]
    }
   ],
   "source": [
    "# Load and clean the IC50 dataset\n",
    "\n",
    "data_path = \"ic50.tsv\"\n",
    "df = pd.read_csv(data_path, sep=\"\t\", low_memory=False)\n",
    "df[\"Standard Value\"] = pd.to_numeric(df[\"Standard Value\"], errors=\"coerce\")\n",
    "\n",
    "df = df.dropna(subset=[\"Standard Value\", \"Smiles\"]).copy()\n",
    "df[\"Binder\"] = (df[\"Standard Value\"] <= 2000).astype(int)\n",
    "\n",
    "unwanted_patterns = [\n",
    "    \".C\", \".Cl\", \".NA+\", \".Na+\", \".na+\", \"[Na+]\", \"Cl.\", \".O=C(O)C(F)(F)F\"\n",
    "]\n",
    "mask = pd.Series(False, index=df.index)\n",
    "for pattern in unwanted_patterns:\n",
    "    mask |= df[\"Smiles\"].str.contains(pattern, regex=False, na=False)\n",
    "removed = int(mask.sum())\n",
    "if removed:\n",
    "    df = df[~mask].copy()\n",
    "    print(f\"Removed {removed} compounds due to unwanted ion patterns\")\n",
    "\n",
    "dupe_mask = df.duplicated(subset=\"Smiles\", keep=\"first\")\n",
    "removed_dupes = int(dupe_mask.sum())\n",
    "if removed_dupes:\n",
    "    df = df[~dupe_mask].copy()\n",
    "    print(f\"Removed {removed_dupes} duplicate SMILES entries\")\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "print(f\"Dataset size after cleaning: {len(df)} compounds\")\n",
    "print(f\"Binder prevalence (<=2000 nM): {df['Binder'].mean()*100:.2f}%\")\n",
    "print(\"Sample molecules:\")\n",
    "for i, (_, row) in enumerate(df.head(5).iterrows(), 1):\n",
    "    smiles_str = row['Smiles'][:70] + \"...\" if len(row['Smiles']) > 70 else row['Smiles']\n",
    "    print(f\"  {i}. {smiles_str} | Binder: {row['Binder']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "412f9d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting SMILES to Morgan fingerprints...\n",
      "Valid fingerprint count: 2039 / 2039\n"
     ]
    }
   ],
   "source": [
    "# Morgan fingerprint generation with logging\n",
    "\n",
    "morgan_generator = GetMorganGenerator(radius=2, fpSize=4096)\n",
    "\n",
    "fingerprint_rows = []\n",
    "fingerprint_indices = []\n",
    "fingerprint_failures = []\n",
    "\n",
    "print(\"Converting SMILES to Morgan fingerprints...\")\n",
    "for idx, smiles in df['Smiles'].items():\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        fingerprint_failures.append((idx, smiles))\n",
    "        continue\n",
    "    try:\n",
    "        fp = morgan_generator.GetFingerprint(mol)\n",
    "        fingerprint_rows.append(np.array(fp, dtype=np.int8))\n",
    "        fingerprint_indices.append(idx)\n",
    "    except Exception as exc:\n",
    "        fingerprint_failures.append((idx, smiles))\n",
    "\n",
    "X_fp = np.asarray(fingerprint_rows, dtype=np.int8)\n",
    "y_fp = df.loc[fingerprint_indices, 'Binder'].to_numpy()\n",
    "print(f\"Valid fingerprint count: {X_fp.shape[0]} / {len(df)}\")\n",
    "if fingerprint_failures:\n",
    "    print(f\"Fingerprint failures: {len(fingerprint_failures)} (showing up to 5)\")\n",
    "    for entry in fingerprint_failures[:5]:\n",
    "        print(f\"  idx={entry[0]} smiles={entry[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db45774a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating RDKit descriptors...\n",
      "Dropped 1 descriptor(s) exceeding float32 limits\n",
      "Descriptor matrix shape: (2039, 216)\n"
     ]
    }
   ],
   "source": [
    "# RDKit descriptor calculation with deferred imputation and logging\n",
    "\n",
    "descriptor_names = [name for name, _ in Descriptors._descList]\n",
    "descriptor_calculator = MoleculeDescriptors.MolecularDescriptorCalculator(descriptor_names)\n",
    "\n",
    "descriptor_rows = []\n",
    "descriptor_indices = []\n",
    "descriptor_failures = []\n",
    "\n",
    "print(\"Calculating RDKit descriptors...\")\n",
    "for idx, smiles in df['Smiles'].items():\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        descriptor_failures.append((idx, smiles))\n",
    "        continue\n",
    "    try:\n",
    "        values = descriptor_calculator.CalcDescriptors(mol)\n",
    "        arr = np.asarray(values, dtype=np.float64)\n",
    "        arr[~np.isfinite(arr)] = np.nan\n",
    "        descriptor_rows.append(arr)\n",
    "        descriptor_indices.append(idx)\n",
    "    except Exception:\n",
    "        descriptor_failures.append((idx, smiles))\n",
    "\n",
    "descriptor_df = pd.DataFrame(descriptor_rows, columns=descriptor_names, index=descriptor_indices)\n",
    "all_nan_cols = descriptor_df.columns[descriptor_df.isna().all()].tolist()\n",
    "if all_nan_cols:\n",
    "    descriptor_df = descriptor_df.drop(columns=all_nan_cols)\n",
    "    print(f\"Dropped {len(all_nan_cols)} all-NaN descriptors\")\n",
    "\n",
    "float32_limit = np.finfo(np.float32).max\n",
    "large_mask = descriptor_df.abs() >= float32_limit\n",
    "if large_mask.any().any():\n",
    "    drop_cols = descriptor_df.columns[large_mask.any(axis=0)].tolist()\n",
    "    descriptor_df = descriptor_df.drop(columns=drop_cols)\n",
    "    print(f\"Dropped {len(drop_cols)} descriptor(s) exceeding float32 limits\")\n",
    "\n",
    "descriptor_df = descriptor_df.sort_index()\n",
    "print(f\"Descriptor matrix shape: {descriptor_df.shape}\")\n",
    "if descriptor_failures:\n",
    "    print(f\"Descriptor failures: {len(descriptor_failures)} (showing up to 5)\")\n",
    "    for entry in descriptor_failures[:5]:\n",
    "        print(f\"  idx={entry[0]} smiles={entry[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20cd77e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Molecules with both fingerprints and descriptors: 2039\n",
      "Combined feature matrix: (2039, 4312) (fp_dim=4096, desc_dim=216)\n"
     ]
    }
   ],
   "source": [
    "# Align molecules with both feature sets\n",
    "\n",
    "fp_idx_map = {idx: pos for pos, idx in enumerate(fingerprint_indices)}\n",
    "desc_idx_map = {idx: pos for pos, idx in enumerate(descriptor_df.index.tolist())}\n",
    "shared_indices = sorted(set(fp_idx_map) & set(desc_idx_map))\n",
    "print(f\"Molecules with both fingerprints and descriptors: {len(shared_indices)}\")\n",
    "if len(shared_indices) < len(df):\n",
    "    missing = len(df) - len(shared_indices)\n",
    "    print(f\"Excluded {missing} compounds lacking at least one feature representation\")\n",
    "\n",
    "X_fp_shared = np.asarray([X_fp[fp_idx_map[idx]] for idx in shared_indices], dtype=np.float32)\n",
    "X_desc_shared = descriptor_df.loc[shared_indices].to_numpy(dtype=np.float32)\n",
    "y_shared = df.loc[shared_indices, 'Binder'].to_numpy()\n",
    "\n",
    "fp_dim = X_fp_shared.shape[1]\n",
    "desc_dim = X_desc_shared.shape[1]\n",
    "X_combined = np.hstack([X_fp_shared, X_desc_shared]).astype(np.float32)\n",
    "print(f\"Combined feature matrix: {X_combined.shape} (fp_dim={fp_dim}, desc_dim={desc_dim})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd65670b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CombinedFeaturePreprocessor(fp_dim=4096)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing transformer and evaluation helpers\n",
    "\n",
    "class CombinedFeaturePreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"Impute + scale descriptor block while leaving fingerprint bits untouched.\"\n",
    "\n",
    "    def __init__(self, fp_dim, epsilon=1e-12):\n",
    "        self.fp_dim = fp_dim\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = np.asarray(X)\n",
    "        desc = X[:, self.fp_dim:]\n",
    "        medians = np.nanmedian(desc, axis=0)\n",
    "        medians = np.where(np.isnan(medians), 0.0, medians)\n",
    "        desc_imputed = np.where(np.isnan(desc), medians, desc)\n",
    "        mean = np.nanmean(desc_imputed, axis=0)\n",
    "        mean = np.where(np.isnan(mean), 0.0, mean)\n",
    "        scale = np.nanstd(desc_imputed, axis=0)\n",
    "        scale = np.where((scale < self.epsilon) | np.isnan(scale), 1.0, scale)\n",
    "        self.medians_ = medians\n",
    "        self.mean_ = mean\n",
    "        self.scale_ = scale\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = np.asarray(X)\n",
    "        fp_block = X[:, :self.fp_dim]\n",
    "        desc = X[:, self.fp_dim:]\n",
    "        desc_imputed = np.where(np.isnan(desc), self.medians_, desc)\n",
    "        desc_scaled = (desc_imputed - self.mean_) / self.scale_\n",
    "        desc_scaled = np.nan_to_num(desc_scaled, copy=False)\n",
    "        return np.hstack([fp_block, desc_scaled]).astype(np.float32)\n",
    "\n",
    "def compute_metrics(y_true, y_prob, threshold=0.5):\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "    return {\n",
    "        \"PR_AUC\": auc(recall, precision),\n",
    "        \"ROC_AUC\": roc_auc_score(y_true, y_prob),\n",
    "        \"MCC\": matthews_corrcoef(y_true, y_pred),\n",
    "        \"BalancedAccuracy\": balanced_accuracy_score(y_true, y_pred),\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "preprocessor = CombinedFeaturePreprocessor(fp_dim=fp_dim)\n",
    "print(preprocessor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "608d2d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RandomForest': {'pipeline': Pipeline(steps=[('preprocess', CombinedFeaturePreprocessor(fp_dim=4096)),\n",
       "                  ('clf',\n",
       "                   RandomForestClassifier(class_weight='balanced',\n",
       "                                          n_estimators=500, n_jobs=-1,\n",
       "                                          random_state=42))]),\n",
       "  'param_distributions': {'clf__n_estimators': [300,\n",
       "    400,\n",
       "    500,\n",
       "    600,\n",
       "    700,\n",
       "    800,\n",
       "    900],\n",
       "   'clf__max_depth': [None, 12, 16, 20, 24, 28],\n",
       "   'clf__min_samples_leaf': [1, 2, 3, 4],\n",
       "   'clf__min_samples_split': [2, 4, 6, 8],\n",
       "   'clf__max_features': ['sqrt', 'log2', 0.3, 0.5]},\n",
       "  'n_iter': 20},\n",
       " 'ExtraTrees': {'pipeline': Pipeline(steps=[('preprocess', CombinedFeaturePreprocessor(fp_dim=4096)),\n",
       "                  ('clf',\n",
       "                   ExtraTreesClassifier(class_weight='balanced', n_estimators=600,\n",
       "                                        n_jobs=-1, random_state=42))]),\n",
       "  'param_distributions': {'clf__n_estimators': [400, 600, 800, 1000],\n",
       "   'clf__max_depth': [None, 12, 16, 20, 28],\n",
       "   'clf__min_samples_leaf': [1, 2, 3, 5],\n",
       "   'clf__max_features': ['sqrt', 'log2', 0.2, 0.4],\n",
       "   'clf__bootstrap': [True, False]},\n",
       "  'n_iter': 20},\n",
       " 'GradientBoosting': {'pipeline': Pipeline(steps=[('preprocess', CombinedFeaturePreprocessor(fp_dim=4096)),\n",
       "                  ('clf',\n",
       "                   GradientBoostingClassifier(random_state=42, subsample=0.8))]),\n",
       "  'param_distributions': {'clf__learning_rate': array([0.01      , 0.03111111, 0.05222222, 0.07333333, 0.09444444,\n",
       "          0.11555556, 0.13666667, 0.15777778, 0.17888889, 0.2       ]),\n",
       "   'clf__n_estimators': [200, 300, 400, 500],\n",
       "   'clf__max_depth': [2, 3, 4, 5],\n",
       "   'clf__min_samples_leaf': [1, 2, 3],\n",
       "   'clf__max_features': ['sqrt', 'log2', 0.3, 0.5]},\n",
       "  'n_iter': 15},\n",
       " 'LogisticRegression': {'pipeline': Pipeline(steps=[('preprocess', CombinedFeaturePreprocessor(fp_dim=4096)),\n",
       "                  ('clf',\n",
       "                   LogisticRegression(class_weight='balanced', l1_ratio=0.5,\n",
       "                                      max_iter=5000, penalty='elasticnet',\n",
       "                                      random_state=42, solver='saga'))]),\n",
       "  'param_distributions': {'clf__C': array([1.00000000e-03, 1.62377674e-03, 2.63665090e-03, 4.28133240e-03,\n",
       "          6.95192796e-03, 1.12883789e-02, 1.83298071e-02, 2.97635144e-02,\n",
       "          4.83293024e-02, 7.84759970e-02, 1.27427499e-01, 2.06913808e-01,\n",
       "          3.35981829e-01, 5.45559478e-01, 8.85866790e-01, 1.43844989e+00,\n",
       "          2.33572147e+00, 3.79269019e+00, 6.15848211e+00, 1.00000000e+01]),\n",
       "   'clf__l1_ratio': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])},\n",
       "  'n_iter': 20}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model registry with class-weighted estimators and hyperparameter spaces\n",
    "\n",
    "MAX_SEARCH_ITER = 12  # cap per-model RandomizedSearch iterations to keep runtime manageable\n",
    "\n",
    "def build_pipeline(estimator):\n",
    "    return Pipeline([\n",
    "        (\"preprocess\", CombinedFeaturePreprocessor(fp_dim=fp_dim)),\n",
    "        (\"clf\", estimator),\n",
    "    ])\n",
    "\n",
    "model_spaces = {\n",
    "    \"RandomForest\": {\n",
    "        \"pipeline\": build_pipeline(RandomForestClassifier(\n",
    "            n_estimators=500,\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "        )),\n",
    "        \"param_distributions\": {\n",
    "            \"clf__n_estimators\": [300, 400, 500, 600, 700, 800, 900],\n",
    "            \"clf__max_depth\": [None, 12, 16, 20, 24, 28],\n",
    "            \"clf__min_samples_leaf\": [1, 2, 3, 4],\n",
    "            \"clf__min_samples_split\": [2, 4, 6, 8],\n",
    "            \"clf__max_features\": ['sqrt', 'log2', 0.3, 0.5],\n",
    "        },\n",
    "        \"n_iter\": 20,\n",
    "    },\n",
    "    \"ExtraTrees\": {\n",
    "        \"pipeline\": build_pipeline(ExtraTreesClassifier(\n",
    "            n_estimators=600,\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "        )),\n",
    "        \"param_distributions\": {\n",
    "            \"clf__n_estimators\": [400, 600, 800, 1000],\n",
    "            \"clf__max_depth\": [None, 12, 16, 20, 28],\n",
    "            \"clf__min_samples_leaf\": [1, 2, 3, 5],\n",
    "            \"clf__max_features\": ['sqrt', 'log2', 0.2, 0.4],\n",
    "            \"clf__bootstrap\": [True, False],\n",
    "        },\n",
    "        \"n_iter\": 20,\n",
    "    },\n",
    "    \"GradientBoosting\": {\n",
    "        \"pipeline\": build_pipeline(GradientBoostingClassifier(\n",
    "            random_state=42,\n",
    "            subsample=0.8,\n",
    "        )),\n",
    "        \"param_distributions\": {\n",
    "            \"clf__learning_rate\": np.linspace(0.01, 0.2, 10),\n",
    "            \"clf__n_estimators\": [200, 300, 400, 500],\n",
    "            \"clf__max_depth\": [2, 3, 4, 5],\n",
    "            \"clf__min_samples_leaf\": [1, 2, 3],\n",
    "            \"clf__max_features\": ['sqrt', 'log2', 0.3, 0.5],\n",
    "        },\n",
    "        \"n_iter\": 15,\n",
    "    },\n",
    "    \"LogisticRegression\": {\n",
    "        \"pipeline\": build_pipeline(LogisticRegression(\n",
    "            penalty='elasticnet',\n",
    "            solver='saga',\n",
    "            class_weight='balanced',\n",
    "            max_iter=5000,\n",
    "            l1_ratio=0.5,\n",
    "            random_state=42,\n",
    "        )),\n",
    "        \"param_distributions\": {\n",
    "            \"clf__C\": np.logspace(-3, 1, 20),\n",
    "            \"clf__l1_ratio\": np.linspace(0.1, 0.9, 9),\n",
    "        },\n",
    "        \"n_iter\": 20,\n",
    "    },\n",
    "}\n",
    "\n",
    "model_spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcbe6aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RandomForest ===\n",
      "Fold 1: ROC-AUC=0.954, PR-AUC=0.970, Train=186.6s (n_iter=12)\n",
      "Fold 2: ROC-AUC=0.987, PR-AUC=0.993, Train=238.1s (n_iter=12)\n",
      "Fold 3: ROC-AUC=0.977, PR-AUC=0.988, Train=275.1s (n_iter=12)\n",
      "Fold 4: ROC-AUC=0.984, PR-AUC=0.992, Train=213.9s (n_iter=12)\n",
      "Fold 5: ROC-AUC=0.980, PR-AUC=0.990, Train=288.4s (n_iter=12)\n",
      "=== ExtraTrees ===\n",
      "Fold 1: ROC-AUC=0.960, PR-AUC=0.974, Train=335.6s (n_iter=12)\n",
      "Fold 2: ROC-AUC=0.985, PR-AUC=0.992, Train=299.5s (n_iter=12)\n",
      "Fold 3: ROC-AUC=0.976, PR-AUC=0.989, Train=252.0s (n_iter=12)\n",
      "Fold 4: ROC-AUC=0.986, PR-AUC=0.993, Train=290.3s (n_iter=12)\n",
      "Fold 5: ROC-AUC=0.982, PR-AUC=0.991, Train=183.0s (n_iter=12)\n",
      "=== GradientBoosting ===\n",
      "Fold 1: ROC-AUC=0.954, PR-AUC=0.970, Train=230.7s (n_iter=12)\n",
      "Fold 2: ROC-AUC=0.986, PR-AUC=0.992, Train=184.9s (n_iter=12)\n",
      "Fold 3: ROC-AUC=0.980, PR-AUC=0.990, Train=39.7s (n_iter=12)\n",
      "Fold 4: ROC-AUC=0.982, PR-AUC=0.991, Train=60.6s (n_iter=12)\n",
      "Fold 5: ROC-AUC=0.976, PR-AUC=0.988, Train=132.5s (n_iter=12)\n",
      "=== LogisticRegression ===\n",
      "Fold 1: ROC-AUC=0.953, PR-AUC=0.969, Train=517.2s (n_iter=12)\n",
      "Fold 2: ROC-AUC=0.982, PR-AUC=0.990, Train=1033.2s (n_iter=12)\n",
      "Fold 3: ROC-AUC=0.970, PR-AUC=0.967, Train=578.2s (n_iter=12)\n",
      "Fold 4: ROC-AUC=0.978, PR-AUC=0.990, Train=1042.9s (n_iter=12)\n",
      "Fold 5: ROC-AUC=0.974, PR-AUC=0.986, Train=922.8s (n_iter=12)\n",
      "Detailed nested CV results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Fold</th>\n",
       "      <th>PR_AUC</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>MCC</th>\n",
       "      <th>BalancedAccuracy</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>TrainSeconds</th>\n",
       "      <th>BestParams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>1</td>\n",
       "      <td>0.970263</td>\n",
       "      <td>0.954110</td>\n",
       "      <td>0.769002</td>\n",
       "      <td>0.882667</td>\n",
       "      <td>0.894608</td>\n",
       "      <td>186.601218</td>\n",
       "      <td>{'clf__n_estimators': 800, 'clf__min_samples_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>2</td>\n",
       "      <td>0.992577</td>\n",
       "      <td>0.986585</td>\n",
       "      <td>0.859794</td>\n",
       "      <td>0.927083</td>\n",
       "      <td>0.936275</td>\n",
       "      <td>238.089143</td>\n",
       "      <td>{'clf__n_estimators': 700, 'clf__min_samples_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>3</td>\n",
       "      <td>0.988295</td>\n",
       "      <td>0.976668</td>\n",
       "      <td>0.840216</td>\n",
       "      <td>0.922664</td>\n",
       "      <td>0.926471</td>\n",
       "      <td>275.121467</td>\n",
       "      <td>{'clf__n_estimators': 700, 'clf__min_samples_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>4</td>\n",
       "      <td>0.991990</td>\n",
       "      <td>0.983546</td>\n",
       "      <td>0.851590</td>\n",
       "      <td>0.929609</td>\n",
       "      <td>0.931373</td>\n",
       "      <td>213.871240</td>\n",
       "      <td>{'clf__n_estimators': 800, 'clf__min_samples_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>5</td>\n",
       "      <td>0.990027</td>\n",
       "      <td>0.980315</td>\n",
       "      <td>0.845688</td>\n",
       "      <td>0.926014</td>\n",
       "      <td>0.928747</td>\n",
       "      <td>288.429672</td>\n",
       "      <td>{'clf__n_estimators': 600, 'clf__min_samples_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>1</td>\n",
       "      <td>0.974353</td>\n",
       "      <td>0.959932</td>\n",
       "      <td>0.789655</td>\n",
       "      <td>0.890271</td>\n",
       "      <td>0.904412</td>\n",
       "      <td>335.579293</td>\n",
       "      <td>{'clf__n_estimators': 800, 'clf__min_samples_l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>2</td>\n",
       "      <td>0.991520</td>\n",
       "      <td>0.985085</td>\n",
       "      <td>0.849321</td>\n",
       "      <td>0.923295</td>\n",
       "      <td>0.931373</td>\n",
       "      <td>299.464597</td>\n",
       "      <td>{'clf__n_estimators': 1000, 'clf__min_samples_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>3</td>\n",
       "      <td>0.988657</td>\n",
       "      <td>0.976326</td>\n",
       "      <td>0.849497</td>\n",
       "      <td>0.932449</td>\n",
       "      <td>0.928922</td>\n",
       "      <td>252.048731</td>\n",
       "      <td>{'clf__n_estimators': 800, 'clf__min_samples_l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>4</td>\n",
       "      <td>0.993079</td>\n",
       "      <td>0.986045</td>\n",
       "      <td>0.872803</td>\n",
       "      <td>0.946338</td>\n",
       "      <td>0.938725</td>\n",
       "      <td>290.329387</td>\n",
       "      <td>{'clf__n_estimators': 1000, 'clf__min_samples_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>5</td>\n",
       "      <td>0.990583</td>\n",
       "      <td>0.982375</td>\n",
       "      <td>0.867728</td>\n",
       "      <td>0.938332</td>\n",
       "      <td>0.938575</td>\n",
       "      <td>182.985635</td>\n",
       "      <td>{'clf__n_estimators': 400, 'clf__min_samples_l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>1</td>\n",
       "      <td>0.970275</td>\n",
       "      <td>0.954425</td>\n",
       "      <td>0.772936</td>\n",
       "      <td>0.879927</td>\n",
       "      <td>0.897059</td>\n",
       "      <td>230.733075</td>\n",
       "      <td>{'clf__n_estimators': 500, 'clf__min_samples_l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>2</td>\n",
       "      <td>0.992136</td>\n",
       "      <td>0.985769</td>\n",
       "      <td>0.854051</td>\n",
       "      <td>0.920455</td>\n",
       "      <td>0.933824</td>\n",
       "      <td>184.922246</td>\n",
       "      <td>{'clf__n_estimators': 200, 'clf__min_samples_l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989739</td>\n",
       "      <td>0.979640</td>\n",
       "      <td>0.828871</td>\n",
       "      <td>0.915720</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>39.708138</td>\n",
       "      <td>{'clf__n_estimators': 400, 'clf__min_samples_l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>4</td>\n",
       "      <td>0.991271</td>\n",
       "      <td>0.981942</td>\n",
       "      <td>0.860968</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.936275</td>\n",
       "      <td>60.593239</td>\n",
       "      <td>{'clf__n_estimators': 500, 'clf__min_samples_l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>5</td>\n",
       "      <td>0.988201</td>\n",
       "      <td>0.976302</td>\n",
       "      <td>0.828052</td>\n",
       "      <td>0.914026</td>\n",
       "      <td>0.921376</td>\n",
       "      <td>132.464529</td>\n",
       "      <td>{'clf__n_estimators': 200, 'clf__min_samples_l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>1</td>\n",
       "      <td>0.969379</td>\n",
       "      <td>0.953114</td>\n",
       "      <td>0.753206</td>\n",
       "      <td>0.875416</td>\n",
       "      <td>0.887255</td>\n",
       "      <td>517.178936</td>\n",
       "      <td>{'clf__l1_ratio': 0.2, 'clf__C': 0.33598182862...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>2</td>\n",
       "      <td>0.989624</td>\n",
       "      <td>0.981745</td>\n",
       "      <td>0.861551</td>\n",
       "      <td>0.933396</td>\n",
       "      <td>0.936275</td>\n",
       "      <td>1033.184637</td>\n",
       "      <td>{'clf__l1_ratio': 0.8, 'clf__C': 3.79269019073...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>3</td>\n",
       "      <td>0.966853</td>\n",
       "      <td>0.969934</td>\n",
       "      <td>0.816765</td>\n",
       "      <td>0.914773</td>\n",
       "      <td>0.914216</td>\n",
       "      <td>578.164350</td>\n",
       "      <td>{'clf__l1_ratio': 0.2, 'clf__C': 0.88586679041...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>4</td>\n",
       "      <td>0.990344</td>\n",
       "      <td>0.978430</td>\n",
       "      <td>0.869600</td>\n",
       "      <td>0.941604</td>\n",
       "      <td>0.938725</td>\n",
       "      <td>1042.873677</td>\n",
       "      <td>{'clf__l1_ratio': 0.1, 'clf__C': 0.54555947811...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>5</td>\n",
       "      <td>0.986401</td>\n",
       "      <td>0.973529</td>\n",
       "      <td>0.806422</td>\n",
       "      <td>0.913617</td>\n",
       "      <td>0.906634</td>\n",
       "      <td>922.835933</td>\n",
       "      <td>{'clf__l1_ratio': 0.6, 'clf__C': 0.54555947811...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Fold    PR_AUC   ROC_AUC       MCC  BalancedAccuracy  \\\n",
       "0         RandomForest     1  0.970263  0.954110  0.769002          0.882667   \n",
       "1         RandomForest     2  0.992577  0.986585  0.859794          0.927083   \n",
       "2         RandomForest     3  0.988295  0.976668  0.840216          0.922664   \n",
       "3         RandomForest     4  0.991990  0.983546  0.851590          0.929609   \n",
       "4         RandomForest     5  0.990027  0.980315  0.845688          0.926014   \n",
       "5           ExtraTrees     1  0.974353  0.959932  0.789655          0.890271   \n",
       "6           ExtraTrees     2  0.991520  0.985085  0.849321          0.923295   \n",
       "7           ExtraTrees     3  0.988657  0.976326  0.849497          0.932449   \n",
       "8           ExtraTrees     4  0.993079  0.986045  0.872803          0.946338   \n",
       "9           ExtraTrees     5  0.990583  0.982375  0.867728          0.938332   \n",
       "10    GradientBoosting     1  0.970275  0.954425  0.772936          0.879927   \n",
       "11    GradientBoosting     2  0.992136  0.985769  0.854051          0.920455   \n",
       "12    GradientBoosting     3  0.989739  0.979640  0.828871          0.915720   \n",
       "13    GradientBoosting     4  0.991271  0.981942  0.860968          0.931818   \n",
       "14    GradientBoosting     5  0.988201  0.976302  0.828052          0.914026   \n",
       "15  LogisticRegression     1  0.969379  0.953114  0.753206          0.875416   \n",
       "16  LogisticRegression     2  0.989624  0.981745  0.861551          0.933396   \n",
       "17  LogisticRegression     3  0.966853  0.969934  0.816765          0.914773   \n",
       "18  LogisticRegression     4  0.990344  0.978430  0.869600          0.941604   \n",
       "19  LogisticRegression     5  0.986401  0.973529  0.806422          0.913617   \n",
       "\n",
       "    Accuracy  TrainSeconds                                         BestParams  \n",
       "0   0.894608    186.601218  {'clf__n_estimators': 800, 'clf__min_samples_s...  \n",
       "1   0.936275    238.089143  {'clf__n_estimators': 700, 'clf__min_samples_s...  \n",
       "2   0.926471    275.121467  {'clf__n_estimators': 700, 'clf__min_samples_s...  \n",
       "3   0.931373    213.871240  {'clf__n_estimators': 800, 'clf__min_samples_s...  \n",
       "4   0.928747    288.429672  {'clf__n_estimators': 600, 'clf__min_samples_s...  \n",
       "5   0.904412    335.579293  {'clf__n_estimators': 800, 'clf__min_samples_l...  \n",
       "6   0.931373    299.464597  {'clf__n_estimators': 1000, 'clf__min_samples_...  \n",
       "7   0.928922    252.048731  {'clf__n_estimators': 800, 'clf__min_samples_l...  \n",
       "8   0.938725    290.329387  {'clf__n_estimators': 1000, 'clf__min_samples_...  \n",
       "9   0.938575    182.985635  {'clf__n_estimators': 400, 'clf__min_samples_l...  \n",
       "10  0.897059    230.733075  {'clf__n_estimators': 500, 'clf__min_samples_l...  \n",
       "11  0.933824    184.922246  {'clf__n_estimators': 200, 'clf__min_samples_l...  \n",
       "12  0.921569     39.708138  {'clf__n_estimators': 400, 'clf__min_samples_l...  \n",
       "13  0.936275     60.593239  {'clf__n_estimators': 500, 'clf__min_samples_l...  \n",
       "14  0.921376    132.464529  {'clf__n_estimators': 200, 'clf__min_samples_l...  \n",
       "15  0.887255    517.178936  {'clf__l1_ratio': 0.2, 'clf__C': 0.33598182862...  \n",
       "16  0.936275   1033.184637  {'clf__l1_ratio': 0.8, 'clf__C': 3.79269019073...  \n",
       "17  0.914216    578.164350  {'clf__l1_ratio': 0.2, 'clf__C': 0.88586679041...  \n",
       "18  0.938725   1042.873677  {'clf__l1_ratio': 0.1, 'clf__C': 0.54555947811...  \n",
       "19  0.906634    922.835933  {'clf__l1_ratio': 0.6, 'clf__C': 0.54555947811...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Nested cross-validation with hyperparameter search\n",
    "\n",
    "outer_splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=42).split(X_combined, y_shared))\n",
    "results_frames = []\n",
    "\n",
    "for model_name, spec in model_spaces.items():\n",
    "    print(f\"=== {model_name} ===\")\n",
    "    fold_rows = []\n",
    "    for fold_id, (train_idx, test_idx) in enumerate(outer_splits, 1):\n",
    "        inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=fold_id * 31)\n",
    "        n_iter = min(spec.get(\"n_iter\", 15), MAX_SEARCH_ITER)\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=spec[\"pipeline\"],\n",
    "            param_distributions=spec[\"param_distributions\"],\n",
    "            n_iter=n_iter,\n",
    "            scoring='roc_auc',\n",
    "            cv=inner_cv,\n",
    "            n_jobs=-1,\n",
    "            refit=True,\n",
    "            random_state=fold_id * 17,\n",
    "            verbose=0,\n",
    "        )\n",
    "        start = time.perf_counter()\n",
    "        search.fit(X_combined[train_idx], y_shared[train_idx])\n",
    "        train_seconds = time.perf_counter() - start\n",
    "        best_model = search.best_estimator_\n",
    "        y_prob = best_model.predict_proba(X_combined[test_idx])[:, 1]\n",
    "        metrics = compute_metrics(y_shared[test_idx], y_prob)\n",
    "        fold_rows.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Fold\": fold_id,\n",
    "            **metrics,\n",
    "            \"TrainSeconds\": train_seconds,\n",
    "            \"BestParams\": search.best_params_,\n",
    "        })\n",
    "        print(\n",
    "            f\"Fold {fold_id}: ROC-AUC={metrics['ROC_AUC']:.3f}, PR-AUC={metrics['PR_AUC']:.3f}, \"\n",
    "            f\"Train={train_seconds:.1f}s (n_iter={n_iter})\"\n",
    "        )\n",
    "    fold_df = pd.DataFrame(fold_rows)\n",
    "    results_frames.append(fold_df)\n",
    "\n",
    "detailed_results = pd.concat(results_frames, ignore_index=True)\n",
    "print(\"Detailed nested CV results:\")\n",
    "display(detailed_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdb47c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance summary (nested CV):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>PR_AUC_mean</th>\n",
       "      <th>PR_AUC_std</th>\n",
       "      <th>ROC_AUC_mean</th>\n",
       "      <th>ROC_AUC_std</th>\n",
       "      <th>MCC_mean</th>\n",
       "      <th>MCC_std</th>\n",
       "      <th>BalancedAccuracy_mean</th>\n",
       "      <th>BalancedAccuracy_std</th>\n",
       "      <th>Accuracy_mean</th>\n",
       "      <th>Accuracy_std</th>\n",
       "      <th>TrainSeconds_mean</th>\n",
       "      <th>TrainSeconds_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>0.987638</td>\n",
       "      <td>0.007597</td>\n",
       "      <td>0.977953</td>\n",
       "      <td>0.010762</td>\n",
       "      <td>0.845801</td>\n",
       "      <td>0.033122</td>\n",
       "      <td>0.926137</td>\n",
       "      <td>0.021743</td>\n",
       "      <td>0.928401</td>\n",
       "      <td>0.014095</td>\n",
       "      <td>272.081528</td>\n",
       "      <td>57.996065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.986630</td>\n",
       "      <td>0.009305</td>\n",
       "      <td>0.976245</td>\n",
       "      <td>0.012912</td>\n",
       "      <td>0.833258</td>\n",
       "      <td>0.036647</td>\n",
       "      <td>0.917607</td>\n",
       "      <td>0.019691</td>\n",
       "      <td>0.923494</td>\n",
       "      <td>0.016555</td>\n",
       "      <td>240.422548</td>\n",
       "      <td>42.177753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>0.986324</td>\n",
       "      <td>0.009097</td>\n",
       "      <td>0.975616</td>\n",
       "      <td>0.012337</td>\n",
       "      <td>0.828976</td>\n",
       "      <td>0.034618</td>\n",
       "      <td>0.912389</td>\n",
       "      <td>0.019430</td>\n",
       "      <td>0.922020</td>\n",
       "      <td>0.015542</td>\n",
       "      <td>129.684245</td>\n",
       "      <td>80.837955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.980520</td>\n",
       "      <td>0.011455</td>\n",
       "      <td>0.971350</td>\n",
       "      <td>0.011152</td>\n",
       "      <td>0.821509</td>\n",
       "      <td>0.046988</td>\n",
       "      <td>0.915761</td>\n",
       "      <td>0.025554</td>\n",
       "      <td>0.916621</td>\n",
       "      <td>0.021464</td>\n",
       "      <td>818.847507</td>\n",
       "      <td>252.919964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model  PR_AUC_mean  PR_AUC_std  ROC_AUC_mean  ROC_AUC_std  \\\n",
       "0          ExtraTrees     0.987638    0.007597      0.977953     0.010762   \n",
       "1        RandomForest     0.986630    0.009305      0.976245     0.012912   \n",
       "2    GradientBoosting     0.986324    0.009097      0.975616     0.012337   \n",
       "3  LogisticRegression     0.980520    0.011455      0.971350     0.011152   \n",
       "\n",
       "   MCC_mean   MCC_std  BalancedAccuracy_mean  BalancedAccuracy_std  \\\n",
       "0  0.845801  0.033122               0.926137              0.021743   \n",
       "1  0.833258  0.036647               0.917607              0.019691   \n",
       "2  0.828976  0.034618               0.912389              0.019430   \n",
       "3  0.821509  0.046988               0.915761              0.025554   \n",
       "\n",
       "   Accuracy_mean  Accuracy_std  TrainSeconds_mean  TrainSeconds_std  \n",
       "0       0.928401      0.014095         272.081528         57.996065  \n",
       "1       0.923494      0.016555         240.422548         42.177753  \n",
       "2       0.922020      0.015542         129.684245         80.837955  \n",
       "3       0.916621      0.021464         818.847507        252.919964  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Aggregate metrics (mean +/- std) per model\n",
    "\n",
    "metric_cols = [\"PR_AUC\", \"ROC_AUC\", \"MCC\", \"BalancedAccuracy\", \"Accuracy\", \"TrainSeconds\"]\n",
    "summary = (detailed_results\n",
    "           .groupby(\"Model\")[metric_cols]\n",
    "           .agg(['mean', 'std'])\n",
    "           .sort_values((\"ROC_AUC\", \"mean\"), ascending=False))\n",
    "summary.columns = [f\"{metric}_{stat}\" for metric, stat in summary.columns]\n",
    "summary = summary.reset_index()\n",
    "print(\"Model performance summary (nested CV):\")\n",
    "display(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "035f8efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Professional summary table (mean ± std across outer folds):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Area Under ROC</th>\n",
       "      <th>Area Under PR</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Matthews CC</th>\n",
       "      <th>Training Time (s)</th>\n",
       "      <th>Representative Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>0.978 ± 0.011</td>\n",
       "      <td>0.988 ± 0.008</td>\n",
       "      <td>0.926 ± 0.022</td>\n",
       "      <td>0.928 ± 0.014</td>\n",
       "      <td>0.846 ± 0.033</td>\n",
       "      <td>272.1 ± 58.0</td>\n",
       "      <td>{'clf__n_estimators': 1000, 'clf__min_samples_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.976 ± 0.013</td>\n",
       "      <td>0.987 ± 0.009</td>\n",
       "      <td>0.918 ± 0.020</td>\n",
       "      <td>0.923 ± 0.017</td>\n",
       "      <td>0.833 ± 0.037</td>\n",
       "      <td>240.4 ± 42.2</td>\n",
       "      <td>{'clf__n_estimators': 700, 'clf__min_samples_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>0.976 ± 0.012</td>\n",
       "      <td>0.986 ± 0.009</td>\n",
       "      <td>0.912 ± 0.019</td>\n",
       "      <td>0.922 ± 0.016</td>\n",
       "      <td>0.829 ± 0.035</td>\n",
       "      <td>129.7 ± 80.8</td>\n",
       "      <td>{'clf__n_estimators': 200, 'clf__min_samples_l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.971 ± 0.011</td>\n",
       "      <td>0.981 ± 0.011</td>\n",
       "      <td>0.916 ± 0.026</td>\n",
       "      <td>0.917 ± 0.021</td>\n",
       "      <td>0.822 ± 0.047</td>\n",
       "      <td>818.8 ± 252.9</td>\n",
       "      <td>{'clf__l1_ratio': 0.8, 'clf__C': 3.79269019073...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model Area Under ROC  Area Under PR Balanced Accuracy  \\\n",
       "0          ExtraTrees  0.978 ± 0.011  0.988 ± 0.008     0.926 ± 0.022   \n",
       "1        RandomForest  0.976 ± 0.013  0.987 ± 0.009     0.918 ± 0.020   \n",
       "2    GradientBoosting  0.976 ± 0.012  0.986 ± 0.009     0.912 ± 0.019   \n",
       "3  LogisticRegression  0.971 ± 0.011  0.981 ± 0.011     0.916 ± 0.026   \n",
       "\n",
       "        Accuracy    Matthews CC Training Time (s)  \\\n",
       "0  0.928 ± 0.014  0.846 ± 0.033      272.1 ± 58.0   \n",
       "1  0.923 ± 0.017  0.833 ± 0.037      240.4 ± 42.2   \n",
       "2  0.922 ± 0.016  0.829 ± 0.035      129.7 ± 80.8   \n",
       "3  0.917 ± 0.021  0.822 ± 0.047     818.8 ± 252.9   \n",
       "\n",
       "                               Representative Params  \n",
       "0  {'clf__n_estimators': 1000, 'clf__min_samples_...  \n",
       "1  {'clf__n_estimators': 700, 'clf__min_samples_s...  \n",
       "2  {'clf__n_estimators': 200, 'clf__min_samples_l...  \n",
       "3  {'clf__l1_ratio': 0.8, 'clf__C': 3.79269019073...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Presentation-style performance table\n",
    "\n",
    "pretty_table = []\n",
    "metric_pairs = [\n",
    "    (\"ROC_AUC\", \"Area Under ROC\"),\n",
    "    (\"PR_AUC\", \"Area Under PR\"),\n",
    "    (\"BalancedAccuracy\", \"Balanced Accuracy\"),\n",
    "    (\"Accuracy\", \"Accuracy\"),\n",
    "    (\"MCC\", \"Matthews CC\"),\n",
    "]\n",
    "for model, group in detailed_results.groupby(\"Model\"):\n",
    "    row = {\"Model\": model}\n",
    "    for metric, label in metric_pairs:\n",
    "        mean_val = group[metric].mean()\n",
    "        std_val = group[metric].std()\n",
    "        row[f\"{label}\"] = f\"{mean_val:.3f} ± {std_val:.3f}\"\n",
    "    mean_time = group[\"TrainSeconds\"].mean()\n",
    "    std_time = group[\"TrainSeconds\"].std()\n",
    "    row[\"Training Time (s)\"] = f\"{mean_time:.1f} ± {std_time:.1f}\"\n",
    "    best_idx = group[\"ROC_AUC\"].idxmax()\n",
    "    row[\"Representative Params\"] = detailed_results.loc[best_idx, \"BestParams\"]\n",
    "    pretty_table.append(row)\n",
    "\n",
    "presentation_df = pd.DataFrame(pretty_table).sort_values(\"Area Under ROC\", ascending=False).reset_index(drop=True)\n",
    "print(\"Professional summary table (mean ± std across outer folds):\")\n",
    "display(presentation_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
